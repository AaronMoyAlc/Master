{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos librerias de pyspark para realizar el preprocesado de los datos\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, isnan, isnull, mean, round\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "from pyspark.ml.feature import IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# importamos funciones auxiliares \n",
    "from filter_datasets import *\n",
    "from process_2008_data import *\n",
    "from models import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crea la sesion de spark\n",
    "spark = SparkSession.builder.appName(\"proyecto\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_configs = [\n",
    "    {\"input\": \"airports.csv\", \"output\": \"filtered_airports.csv\", \"columns\": [\"iata\"]},\n",
    "    {\"input\": \"carriers.csv\", \"output\": \"filtered_carriers.csv\", \"columns\": [\"Code\"]},\n",
    "    {\n",
    "        \"input\": \"plane-data.csv\",\n",
    "        \"output\": \"filtered_plane_data.csv\",\n",
    "        \"columns\": [\"tailnum\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Process each file\n",
    "for config in file_configs:\n",
    "    filter_columns(config[\"input\"], config[\"output\"], config[\"columns\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output file paths\n",
    "input_2008_file = \"2008.csv\" \n",
    "input_plane_file = \"plane-data.csv\"\n",
    "output_file = \"processed_2008.csv\"\n",
    "#original_col = [Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay]\n",
    "# Run the function\n",
    "process_2008_data(input_2008_file, input_plane_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "# se carga el dataset\n",
    "df = spark.read.csv(\"processed_2008.csv\", header=True, inferSchema=True)\n",
    "cols = (\n",
    "    df.columns\n",
    ")  # Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,TailNum,IssueDate,CRSElapsedTime,ArrDelay,DepDelay,Origin,Dest,Cancelled\n",
    "\n",
    "target = \"ArrDelay\"\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprueba si hay valores nulos\n",
    "for col in cols:\n",
    "    print(col, df.filter(df[col].isNull()).count())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se entrenan los modelos\n",
    "# se crea un vector con las columnas que se van a utilizar\n",
    "features = df.columns\n",
    "features.remove(\"ArrDelay\")\n",
    "\n",
    "# se convierte la variable target a numerica\n",
    "\n",
    "df = df.withColumn(\"ArrDelay\", df[\"ArrDelay\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"ArrDelay\", when(col(\"ArrDelay\") > 15, 1).otherwise(0))\n",
    "df = df.withColumn(\"ArrDelay\", df[\"ArrDelay\"].cast(IntegerType()))\n",
    "\n",
    "# se convierten las variables categoricas a numericas (tailnum, UniqueCarrier, Origin, Dest, )\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=column, outputCol=column + \"_index\").fit(df)\n",
    "    for column in [\"UniqueCarrier\", \"TailNum\", \"Origin\", \"Dest\"]\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df = pipeline.fit(df).transform(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "model = estimate_lr(df, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans\n",
    "k = 2\n",
    "model = estimate_kmeans(df, features, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
