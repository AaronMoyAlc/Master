{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hD8V7OZQ9jQN"
      },
      "outputs": [],
      "source": [
        "# importamos librerias de pyspark para realizar el preprocesado de los datos\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, count, isnan, isnull, mean, round\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import OneVsRest\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import CrossValidatorModel\n",
        "from pyspark.ml.feature import IndexToString\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# importamos funciones auxiliares\n",
        "# from filter_datasets import *\n",
        "# from process_2008_data import *\n",
        "from models import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aVdthbGa9jQP"
      },
      "outputs": [],
      "source": [
        "# se crea la sesion de spark\n",
        "spark = SparkSession.builder.appName(\"proyecto\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reWImfUX9jQP"
      },
      "outputs": [],
      "source": [
        "file_configs = [\n",
        "    {\"input\": \"airports.csv\", \"output\": \"filtered_airports.csv\", \"columns\": [\"iata\"]},\n",
        "    {\"input\": \"carriers.csv\", \"output\": \"filtered_carriers.csv\", \"columns\": [\"Code\"]},\n",
        "    {\n",
        "        \"input\": \"plane-data.csv\",\n",
        "        \"output\": \"filtered_plane_data.csv\",\n",
        "        \"columns\": [\"tailnum\"],\n",
        "    },\n",
        "]\n",
        "\n",
        "# Process each file\n",
        "for config in file_configs:\n",
        "    filter_columns(config[\"input\"], config[\"output\"], config[\"columns\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-b-6HR_9jQP"
      },
      "outputs": [],
      "source": [
        "# Input and output file paths\n",
        "input_2008_file = \"2008.csv\"\n",
        "input_plane_file = \"plane-data.csv\"\n",
        "output_file = \"processed_2008.csv\"\n",
        "#original_col = [Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay]\n",
        "# Run the function\n",
        "process_2008_data(input_2008_file, input_plane_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUSCNKU_9jQP",
        "outputId": "000471c0-ebf1-4292-cdcb-fe56ef283582"
      },
      "outputs": [],
      "source": [
        "# EDA\n",
        "# se carga el dataset\n",
        "df = spark.read.csv(\"processed_2008.csv\", header=True, inferSchema=True)\n",
        "# Mostrar esquema de las columnas\n",
        "df.printSchema()\n",
        "\n",
        "# Mostrar los primeros registros\n",
        "df.show(5)\n",
        "\n",
        "# 1. Resumen estadístico de las columnas numéricas\n",
        "numerical_cols = [\n",
        "    \"Month\",\n",
        "    \"DayofMonth\",\n",
        "    \"DayOfWeek\",\n",
        "    \"DepTime\",\n",
        "    \"CRSDepTime\",\n",
        "    \"CRSArrTime\",\n",
        "    \"CRSElapsedTime\",\n",
        "    \"ArrDelay\",\n",
        "    \"DepDelay\",\n",
        "]\n",
        "df.select(numerical_cols).describe().show()\n",
        "\n",
        "# 2. Inspección de valores nulos o faltantes\n",
        "missing_data = df.select(\n",
        "    [count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]\n",
        ")\n",
        "print(\"Cantidad de valores nulos por columna:\")\n",
        "missing_data.show()\n",
        "\n",
        "# 3. Inspección de columnas categóricas\n",
        "categorical_cols = [\"UniqueCarrier\", \"TailNum\", \"Origin\", \"Dest\"]\n",
        "for col_name in categorical_cols:\n",
        "    print(f\"Distribución de valores únicos para la columna {col_name}:\")\n",
        "    df.groupBy(col_name).count().orderBy(\"count\", ascending=False).show(5)\n",
        "\n",
        "# 4. Inspección específica de la variable objetivo (ArrDelay)\n",
        "print(\"Estadísticas descriptivas de la variable objetivo (ArrDelay):\")\n",
        "df.select(\"ArrDelay\").describe().show()\n",
        "\n",
        "# 5. Identificar correlaciones básicas (opcional, solo entre columnas numéricas)\n",
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"features\")\n",
        "vector_df = assembler.transform(df).select(\"features\")\n",
        "correlation_matrix = Correlation.corr(vector_df, \"features\").head()[0]\n",
        "print(\"Matriz de correlación:\")\n",
        "print(correlation_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aGX07ER9jQQ",
        "outputId": "b4644d20-e829-4313-cc89-53e6cc7ab29b"
      },
      "outputs": [],
      "source": [
        "# comprueba si hay valores nulos\n",
        "for col in cols:\n",
        "    print(col, df.filter(df[col].isNull()).count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2NuelG61-vp5"
      },
      "outputs": [],
      "source": [
        "# elimeinamos los nulos en la columna ArrDelay\n",
        "df = df.filter(df[\"ArrDelay\"].isNotNull())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7w9lHewE-_ps",
        "outputId": "a1814a48-7cdd-4006-c4dd-8b3288e6cdd9"
      },
      "outputs": [],
      "source": [
        "# comprueba si hay valores nulos\n",
        "for col in cols:\n",
        "    print(col, df.filter(df[col].isNull()).count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, year, avg, when, lit\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
        "\n",
        "current_year = 2008  # Ajustar según el año actual\n",
        "data = data.withColumn(\n",
        "    \"PlaneAge\",\n",
        "    when(col(\"IssueDate\").isNotNull(), current_year - year(col(\"IssueDate\"))).otherwise(\n",
        "        None\n",
        "    ),\n",
        ")\n",
        "# Rellenar nulos en PlaneAge con la media\n",
        "avg_age = data.select(avg(\"PlaneAge\")).first()[0]\n",
        "data = data.withColumn(\n",
        "    \"PlaneAge\", when(col(\"PlaneAge\").isNull(), avg_age).otherwise(col(\"PlaneAge\"))\n",
        ")\n",
        "\n",
        "# Convertir columnas categóricas a índices numéricos\n",
        "categorical_columns = [\"UniqueCarrier\", \"Origin\", \"Dest\"]\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=col, outputCol=f\"{col}_Index\") for col in categorical_columns\n",
        "]\n",
        "for indexer in indexers:\n",
        "    data = indexer.fit(data).transform(data)\n",
        "\n",
        "# Eliminar columnas originales categóricas\n",
        "data = data.drop(*categorical_columns)\n",
        "\n",
        "# Normalizar los valores\n",
        "feature_columns = [\n",
        "    \"Month\",\n",
        "    \"DayofMonth\",\n",
        "    \"DayOfWeek\",\n",
        "    \"DepTime\",\n",
        "    \"CRSDepTime\",\n",
        "    \"CRSArrTime\",\n",
        "    \"CRSElapsedTime\",\n",
        "    \"DepDelay\",\n",
        "    \"Cancelled\",\n",
        "    \"PlaneAge\",\n",
        "    \"UniqueCarrier_Index\",\n",
        "    \"Origin_Index\",\n",
        "    \"Dest_Index\",\n",
        "]\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features_assembled\")\n",
        "data = assembler.transform(data)\n",
        "\n",
        "scaler = MinMaxScaler(inputCol=\"features_assembled\", outputCol=\"features\")\n",
        "scaler_model = scaler.fit(data)\n",
        "data = scaler_model.transform(data)\n",
        "\n",
        "# Seleccionar columnas finales (incluye la normalizada y la variable objetivo)\n",
        "df = data.select(\"features\", \"ArrDelay\")\n",
        "\n",
        "# Mostrar algunas filas del conjunto preprocesado\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Y-dasLUJ9jQQ",
        "outputId": "5cec1919-a76b-4d8a-a0f6-8fd6ddefedec"
      },
      "outputs": [],
      "source": [
        "# Se crea un vector con las columnas que se van a utilizar\n",
        "features = df.columns\n",
        "features.remove(\"ArrDelay\")\n",
        "from pyspark.sql.functions import col\n",
        "# Se convierte la variable target a numérica\n",
        "df = df.withColumn(\"ArrDelay\", df[\"ArrDelay\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"ArrDelay\", when(col(\"ArrDelay\") > 15, 1).otherwise(0)) # 1 si el vuelo se retrasó más de 15 minutos, 0 si no\n",
        "df = df.withColumn(\"ArrDelay\", df[\"ArrDelay\"].cast(IntegerType()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFzxywhED1Jr",
        "outputId": "fd3e2ec2-8e91-4178-e048-994a72970d69"
      },
      "outputs": [],
      "source": [
        "print(\"Primeras filas del dataframe:\")\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "X485XwoLD9o1"
      },
      "outputs": [],
      "source": [
        "# # eliminamos las filas que hemos indexado\n",
        "# df = df.drop(*[\"UniqueCarrier\", \"TailNum\", \"Origin\", \"Dest\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOwXztKOESSO"
      },
      "outputs": [],
      "source": [
        "# Dividir los datos en entrenamiento y prueba\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 1: Regresión Lineal\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"ArrDelay\")\n",
        "param_grid_lr = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "    .build()\n",
        ")\n",
        "\n",
        "best_lr_model, rmse_lr = train_and_evaluate(lr, param_grid_lr, train_data, test_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 2: Bosques Aleatorios\n",
        "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"ArrDelay\")\n",
        "param_grid_rf = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(rf.numTrees, [50, 100])\n",
        "    .addGrid(rf.maxDepth, [5, 10])\n",
        "    .build()\n",
        ")\n",
        "\n",
        "best_rf_model, rmse_rf = train_and_evaluate(rf, param_grid_rf, train_data, test_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 3: Gradient Boosted Trees\n",
        "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"ArrDelay\")\n",
        "param_grid_gbt = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(gbt.maxIter, [10, 50])\n",
        "    .addGrid(gbt.maxDepth, [5, 10])\n",
        "    .build()\n",
        ")\n",
        "\n",
        "best_gbt_model, rmse_gbt = train_and_evaluate(\n",
        "    gbt, param_grid_gbt, train_data, test_data\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparar modelos\n",
        "results = [\n",
        "    (\"Linear Regression\", rmse_lr),\n",
        "    (\"Random Forest\", rmse_rf),\n",
        "    (\"Gradient Boosted Trees\", rmse_gbt),\n",
        "]\n",
        "results_sorted = sorted(results, key=lambda x: x[1])  # Ordenar por menor RMSE\n",
        "\n",
        "print(\"Model Comparison (RMSE):\")\n",
        "for model_name, rmse in results_sorted:\n",
        "    print(f\"{model_name}: {rmse:.3f}\")\n",
        "\n",
        "# Elegir el mejor modelo\n",
        "best_model_name, best_rmse = results_sorted[0]\n",
        "print(f\"\\nBest Model: {best_model_name} with RMSE = {best_rmse:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igZh7yz89jQQ",
        "outputId": "784b716d-b640-4797-8519-1e56308d18a6"
      },
      "outputs": [],
      "source": [
        "# linear regression\n",
        "# Uso de funciones del archivo models.py\n",
        "from models import estimate_lr, estimate_kmeans\n",
        "input_columns_lr = [col for col in df.columns if (col != \"ArrDelay\" and col != \"IssueDate\")]\n",
        "print(input_columns_lr)\n",
        "model = estimate_lr(df, input_columns_lr, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1-dfMVB9jQQ",
        "outputId": "ee30a058-cbdd-4428-a81d-7260e73acc36"
      },
      "outputs": [],
      "source": [
        "# kmeans\n",
        "input_columns_kmeans = [col for col in df.columns if (col != \"ArrDelay\" and col != \"IssueDate\")]\n",
        "k = 3\n",
        "kmeans_model = estimate_kmeans(df, input_columns_kmeans, k)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
